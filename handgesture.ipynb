{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-07 15:35:42.813393: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-07 15:35:42.813667: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error converting shape to a TensorShape: invalid literal for int() with base 10: 'class_name'.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m~/opt/anaconda3/envs/tankyu/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001B[0m in \u001B[0;36mmake_shape\u001B[0;34m(v, arg_name)\u001B[0m\n\u001B[1;32m    205\u001B[0m   \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 206\u001B[0;31m     \u001B[0mshape\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtensor_shape\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mas_shape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mv\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    207\u001B[0m   \u001B[0;32mexcept\u001B[0m \u001B[0mTypeError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/tankyu/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\u001B[0m in \u001B[0;36mas_shape\u001B[0;34m(shape)\u001B[0m\n\u001B[1;32m   1215\u001B[0m   \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1216\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mTensorShape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1217\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/tankyu/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, dims)\u001B[0m\n\u001B[1;32m    775\u001B[0m         \u001B[0;31m# Got a list of dimensions\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 776\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dims\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mas_dimension\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0md\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0md\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdims_iter\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    777\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/tankyu/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    775\u001B[0m         \u001B[0;31m# Got a list of dimensions\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 776\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dims\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mas_dimension\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0md\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0md\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdims_iter\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    777\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/tankyu/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\u001B[0m in \u001B[0;36mas_dimension\u001B[0;34m(value)\u001B[0m\n\u001B[1;32m    717\u001B[0m   \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 718\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mDimension\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    719\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/tankyu/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, value)\u001B[0m\n\u001B[1;32m    192\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 193\u001B[0;31m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_value\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    194\u001B[0m       if (not isinstance(value, compat.bytes_or_text_types) and\n",
      "\u001B[0;31mValueError\u001B[0m: invalid literal for int() with base 10: 'class_name'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/_c/yfkk358d56n84q0rcrprvkc00000gn/T/ipykernel_4367/3997721819.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;31m# Load the gesture recognizer model\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 8\u001B[0;31m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mload_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'mp_hand_gesture'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      9\u001B[0m \u001B[0;31m# Load class names\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0mf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'gesture.names'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'r'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/tankyu/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\u001B[0m in \u001B[0;36mload_model\u001B[0;34m(filepath, custom_objects, compile)\u001B[0m\n\u001B[1;32m    148\u001B[0m   \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msix\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstring_types\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    149\u001B[0m     \u001B[0mloader_impl\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparse_saved_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 150\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0msaved_model_load\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcompile\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    151\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    152\u001B[0m   raise IOError(\n",
      "\u001B[0;32m~/opt/anaconda3/envs/tankyu/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(path, compile)\u001B[0m\n\u001B[1;32m     84\u001B[0m   \u001B[0;31m# TODO(kathywu): Add saving/loading of optimizer, compiled losses and metrics.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     85\u001B[0m   \u001B[0;31m# TODO(kathywu): Add code to load from objects that contain all endpoints\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 86\u001B[0;31m   \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf_load\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_internal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mloader_cls\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mKerasObjectLoader\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     87\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     88\u001B[0m   \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRevivedModel\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mcompile\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/tankyu/lib/python3.7/site-packages/tensorflow_core/python/saved_model/load.py\u001B[0m in \u001B[0;36mload_internal\u001B[0;34m(export_dir, tags, loader_cls)\u001B[0m\n\u001B[1;32m    539\u001B[0m       loader = loader_cls(object_graph_proto,\n\u001B[1;32m    540\u001B[0m                           \u001B[0msaved_model_proto\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 541\u001B[0;31m                           export_dir)\n\u001B[0m\u001B[1;32m    542\u001B[0m       \u001B[0mroot\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mloader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    543\u001B[0m     \u001B[0mroot\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtensorflow_version\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmeta_graph_def\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmeta_info_def\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtensorflow_version\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/tankyu/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    101\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    102\u001B[0m     \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mKerasObjectLoader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 103\u001B[0;31m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_finalize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    104\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    105\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m_finalize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/tankyu/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\u001B[0m in \u001B[0;36m_finalize\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    125\u001B[0m             \u001B[0mnode\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_layers\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    126\u001B[0m           \u001B[0;32mfor\u001B[0m \u001B[0mlayer\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mnode\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeras_api\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 127\u001B[0;31m             \u001B[0mnode\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlayer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    128\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    129\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mnode\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/tankyu/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\u001B[0m in \u001B[0;36m_method_wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    455\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_self_setattr_tracking\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m  \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    456\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 457\u001B[0;31m       \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmethod\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    458\u001B[0m     \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    459\u001B[0m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_self_setattr_tracking\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mprevious_value\u001B[0m  \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/tankyu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py\u001B[0m in \u001B[0;36madd\u001B[0;34m(self, layer)\u001B[0m\n\u001B[1;32m    172\u001B[0m           \u001B[0;31m# Instantiate an input layer.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    173\u001B[0m           x = input_layer.Input(\n\u001B[0;32m--> 174\u001B[0;31m               batch_shape=batch_shape, dtype=dtype, name=layer.name + '_input')\n\u001B[0m\u001B[1;32m    175\u001B[0m           \u001B[0;31m# This will build the current layer\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    176\u001B[0m           \u001B[0;31m# and create the node connecting the current layer\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/tankyu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/input_layer.py\u001B[0m in \u001B[0;36mInput\u001B[0;34m(shape, batch_size, name, dtype, sparse, tensor, ragged, **kwargs)\u001B[0m\n\u001B[1;32m    261\u001B[0m         \u001B[0msparse\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msparse\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    262\u001B[0m         \u001B[0mragged\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mragged\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 263\u001B[0;31m         input_tensor=tensor)\n\u001B[0m\u001B[1;32m    264\u001B[0m   \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    265\u001B[0m     input_layer = InputLayer(\n",
      "\u001B[0;32m~/opt/anaconda3/envs/tankyu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/input_layer.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, input_shape, batch_size, dtype, input_tensor, sparse, name, ragged, **kwargs)\u001B[0m\n\u001B[1;32m    123\u001B[0m             \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    124\u001B[0m             \u001B[0msparse\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msparse\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 125\u001B[0;31m             ragged=ragged)\n\u001B[0m\u001B[1;32m    126\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    127\u001B[0m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_placeholder\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/tankyu/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001B[0m in \u001B[0;36mplaceholder\u001B[0;34m(shape, ndim, dtype, sparse, name, ragged)\u001B[0m\n\u001B[1;32m   1055\u001B[0m           name=name)\n\u001B[1;32m   1056\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1057\u001B[0;31m       \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0marray_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplaceholder\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mshape\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1058\u001B[0m   \u001B[0;32mreturn\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1059\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/tankyu/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001B[0m in \u001B[0;36mplaceholder\u001B[0;34m(dtype, shape, name)\u001B[0m\n\u001B[1;32m   2628\u001B[0m                        \"eager execution.\")\n\u001B[1;32m   2629\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2630\u001B[0;31m   \u001B[0;32mreturn\u001B[0m \u001B[0mgen_array_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplaceholder\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mshape\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2631\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2632\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/tankyu/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\u001B[0m in \u001B[0;36mplaceholder\u001B[0;34m(dtype, shape, name)\u001B[0m\n\u001B[1;32m   6667\u001B[0m   \u001B[0;32mif\u001B[0m \u001B[0mshape\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6668\u001B[0m     \u001B[0mshape\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 6669\u001B[0;31m   \u001B[0mshape\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_execute\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmake_shape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"shape\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   6670\u001B[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001B[1;32m   6671\u001B[0m         \"Placeholder\", dtype=dtype, shape=shape, name=name)\n",
      "\u001B[0;32m~/opt/anaconda3/envs/tankyu/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001B[0m in \u001B[0;36mmake_shape\u001B[0;34m(v, arg_name)\u001B[0m\n\u001B[1;32m    209\u001B[0m   \u001B[0;32mexcept\u001B[0m \u001B[0mValueError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    210\u001B[0m     raise ValueError(\"Error converting %s to a TensorShape: %s.\" % (arg_name,\n\u001B[0;32m--> 211\u001B[0;31m                                                                     e))\n\u001B[0m\u001B[1;32m    212\u001B[0m   \u001B[0;32mif\u001B[0m \u001B[0mshape\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mndims\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    213\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: Error converting shape to a TensorShape: invalid literal for int() with base 10: 'class_name'."
     ]
    }
   ],
   "source": [
    "from tensorflow_core.python.ops.signal.shape_ops import frame\n",
    "from tensorflow_core.python.keras.models import load_model\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Load the gesture recognizer model\n",
    "model = load_model('mp_hand_gesture')\n",
    "# Load class names\n",
    "f = open('gesture.names', 'r')\n",
    "classNames = f.read().split('\\n')\n",
    "f.close()\n",
    "print(classNames)\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# For static images:\n",
    "IMAGE_FILES = []\n",
    "with mp_hands.Hands(\n",
    "    static_image_mode=True,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5) as hands:\n",
    "    for idx, file in enumerate(IMAGE_FILES):\n",
    "        # Read an image, flip it around y-axis for correct handedness output (see\n",
    "        # above).\n",
    "        image = cv2.flip(cv2.imread(file), 1)\n",
    "        # Convert the BGR image to RGB before processing.\n",
    "        results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # Print handedness and draw hand landmarks on the image.\n",
    "        print('Handedness:', results.multi_handedness)\n",
    "        if not results.multi_hand_landmarks:\n",
    "            continue\n",
    "        image_height, image_width, _ = image.shape\n",
    "        annotated_image = image.copy()\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            print('hand_landmarks:', hand_landmarks)\n",
    "            print(\n",
    "                f'Index finger tip coordinates: (',\n",
    "                f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x * image_width}, '\n",
    "                f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y * image_height})'\n",
    "            )\n",
    "            mp_drawing.draw_landmarks(\n",
    "                annotated_image,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "        cv2.imwrite('/tmp/annotated_image' + str(idx) + '.png', cv2.flip(annotated_image, 1))\n",
    "        # Draw hand world landmarks.\n",
    "        if not results.multi_hand_world_landmarks:\n",
    "            continue\n",
    "        for hand_world_landmarks in results.multi_hand_world_landmarks:\n",
    "            mp_drawing.plot_landmarks(hand_world_landmarks, mp_hands.HAND_CONNECTIONS, azimuth=5)\n",
    "\n",
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_hands.Hands(\n",
    "    model_complexity=0,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ") as hands:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            # If loading a video, use 'break' instead of 'continue'.\n",
    "            continue\n",
    "\n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # Draw the hand annotations on the image.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        if results.multi_hand_landmarks:\n",
    "            landmarks = []\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "                # Predict gesture in Hand Gesture Recognition project\n",
    "                prediction = model.predict([landmarks])\n",
    "                print(prediction)\n",
    "                classID = np.argmax(prediction)\n",
    "                className = classNames[classID]\n",
    "        # show the prediction on the frame\n",
    "        cv2.putText(frame, className, (10, 50), cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,255), 2, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "        # Flip the image horizontally for a selfie-view display.\n",
    "        cv2.imshow('MediaPipe Hands', cv2.flip(image, 1))\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "2021-12-07 15:32:17.930 python[4351:117446] _TIPropertyValueIsValid called with 4 on nil context!\n",
      "2021-12-07 15:32:17.930 python[4351:117446] imkxpc_getApplicationProperty:reply: called with incorrect property value 4, bailing.\n",
      "2021-12-07 15:32:17.930 python[4351:117446] Text input context does not respond to _valueForTIProperty:\n",
      "2021-12-07 15:32:17.930 python[4351:117446] _TIPropertyValueIsValid called with 4 on nil context!\n",
      "2021-12-07 15:32:17.930 python[4351:117446] imkxpc_getApplicationProperty:reply: called with incorrect property value 4, bailing.\n",
      "2021-12-07 15:32:17.930 python[4351:117446] Text input context does not respond to _valueForTIProperty:\n",
      "2021-12-07 15:32:43.849 python[4351:117446] _TIPropertyValueIsValid called with 4 on nil context!\n",
      "2021-12-07 15:32:43.849 python[4351:117446] imkxpc_getApplicationProperty:reply: called with incorrect property value 4, bailing.\n",
      "2021-12-07 15:32:43.849 python[4351:117446] Text input context does not respond to _valueForTIProperty:\n",
      "2021-12-07 15:32:43.913 python[4351:117446] _TIPropertyValueIsValid called with 4 on nil context!\n",
      "2021-12-07 15:32:43.913 python[4351:117446] imkxpc_getApplicationProperty:reply: called with incorrect property value 4, bailing.\n",
      "2021-12-07 15:32:43.913 python[4351:117446] Text input context does not respond to _valueForTIProperty:\n",
      "2021-12-07 15:33:27.099 python[4351:117446] _TIPropertyValueIsValid called with 4 on nil context!\n",
      "2021-12-07 15:33:27.099 python[4351:117446] imkxpc_getApplicationProperty:reply: called with incorrect property value 4, bailing.\n",
      "2021-12-07 15:33:27.100 python[4351:117446] Text input context does not respond to _valueForTIProperty:\n",
      "2021-12-07 15:33:27.100 python[4351:117446] _TIPropertyValueIsValid called with 4 on nil context!\n",
      "2021-12-07 15:33:27.100 python[4351:117446] imkxpc_getApplicationProperty:reply: called with incorrect property value 4, bailing.\n",
      "2021-12-07 15:33:27.100 python[4351:117446] Text input context does not respond to _valueForTIProperty:\n",
      "2021-12-07 15:33:29.095 python[4351:117446] _TIPropertyValueIsValid called with 4 on nil context!\n",
      "2021-12-07 15:33:29.095 python[4351:117446] imkxpc_getApplicationProperty:reply: called with incorrect property value 4, bailing.\n",
      "2021-12-07 15:33:29.095 python[4351:117446] Text input context does not respond to _valueForTIProperty:\n",
      "2021-12-07 15:33:29.154 python[4351:117446] _TIPropertyValueIsValid called with 4 on nil context!\n",
      "2021-12-07 15:33:29.154 python[4351:117446] imkxpc_getApplicationProperty:reply: called with incorrect property value 4, bailing.\n",
      "2021-12-07 15:33:29.154 python[4351:117446] Text input context does not respond to _valueForTIProperty:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/_c/yfkk358d56n84q0rcrprvkc00000gn/T/ipykernel_4351/2365622152.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     53\u001B[0m     min_tracking_confidence=0.5) as hands:\n\u001B[1;32m     54\u001B[0m   \u001B[0;32mwhile\u001B[0m \u001B[0mcap\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0misOpened\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 55\u001B[0;31m     \u001B[0msuccess\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mimage\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcap\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     56\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0msuccess\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     57\u001B[0m       \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Ignoring empty camera frame.\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# For static images:\n",
    "IMAGE_FILES = []\n",
    "with mp_hands.Hands(\n",
    "    static_image_mode=True,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5) as hands:\n",
    "  for idx, file in enumerate(IMAGE_FILES):\n",
    "    # Read an image, flip it around y-axis for correct handedness output (see\n",
    "    # above).\n",
    "    image = cv2.flip(cv2.imread(file), 1)\n",
    "    # Convert the BGR image to RGB before processing.\n",
    "    results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Print handedness and draw hand landmarks on the image.\n",
    "    print('Handedness:', results.multi_handedness)\n",
    "    if not results.multi_hand_landmarks:\n",
    "      continue\n",
    "    image_height, image_width, _ = image.shape\n",
    "    annotated_image = image.copy()\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "      print('hand_landmarks:', hand_landmarks)\n",
    "      print(\n",
    "          f'Index finger tip coordinates: (',\n",
    "          f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x * image_width}, '\n",
    "          f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y * image_height})'\n",
    "      )\n",
    "      mp_drawing.draw_landmarks(\n",
    "          annotated_image,\n",
    "          hand_landmarks,\n",
    "          mp_hands.HAND_CONNECTIONS,\n",
    "          mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "          mp_drawing_styles.get_default_hand_connections_style())\n",
    "    cv2.imwrite(\n",
    "        '/tmp/annotated_image' + str(idx) + '.png', cv2.flip(annotated_image, 1))\n",
    "    # Draw hand world landmarks.\n",
    "    if not results.multi_hand_world_landmarks:\n",
    "      continue\n",
    "    for hand_world_landmarks in results.multi_hand_world_landmarks:\n",
    "      mp_drawing.plot_landmarks(\n",
    "        hand_world_landmarks, mp_hands.HAND_CONNECTIONS, azimuth=5)\n",
    "\n",
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_hands.Hands(\n",
    "    model_complexity=0,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as hands:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image)\n",
    "\n",
    "    # Draw the hand annotations on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    if results.multi_hand_landmarks:\n",
    "      for hand_landmarks in results.multi_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            hand_landmarks,\n",
    "            mp_hands.HAND_CONNECTIONS,\n",
    "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "            mp_drawing_styles.get_default_hand_connections_style())\n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    cv2.imshow('MediaPipe Hands', cv2.flip(image, 1))\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "      break\n",
    "cap.release()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "conda_tankyu",
   "language": "python",
   "display_name": "Environment (conda_tankyu)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}